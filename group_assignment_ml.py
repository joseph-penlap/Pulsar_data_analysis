# -*- coding: utf-8 -*-
"""Group_Assignment_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIW_Ixh9h2Hui04GcW0piFvLqiTRBb4t

Group work

MEMBERS:

* Joseph Penlap
* Josue Nguinabe
* Lydie Gaelle
* Nathalie Jadot
* Paule Comfort
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
#warnings.filterwarnings("ignore")



"""The data contains 2 subsets:

    1- pulsar_data_train : to use it to train the model

    2- pulsar_data_test : to use it to test the model

"""

pulsar_data=pd.read_csv("pulsar_data_train.csv")
#test_df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Group_Assignment_ML/pulsar_data_test.csv")



pulsar_data.head()   # sample of training data

pulsar_data.shape

pulsar_data.shape

"""## EDA"""

pulsar_data.describe()

pulsar_data.info()    # get information about the data(data types, null)

pulsar_data.info()       # get information about the test data

"""
Data modeling (train sample)

    1- correct columns names
    2- remove duplicates
    3- remove null
    4- remove outliers"""

#rename the columns of the data frame so that it can be easil
pulsar_data.columns = ['IP_mean', 'IP_std', 'IP_kurtosis', 'IP_skewness','DM-SNR_mean','DM-SNR_std', 'DM-SNR_kurtosis','DM-SNR_skewness', 'target']

#check the changes
pulsar_data.info()

pulsar_data.info()

#count number of duplicated rows in train data
print('There is: ' + str(pulsar_data.duplicated().sum()) + " duplicated rows in this training dataset.")

# Present the distribution of each column
pulsar_data.hist(bins = 120, figsize=(14,16))
plt.show()

# Count null in each column
pulsar_data.isnull().sum()

# heat map for null values
sns.heatmap(pulsar_data.isnull())

pulsar_data.dropna(inplace=True)

pulsar_data.isnull().sum()

pulsar_data.head()

"""# Outliers"""

pulsar_data.plot(kind = "box" , subplots = True , figsize = (18,15) ,  layout = (3,4))
plt.show()

"""## Split target feature"""

new_data = pulsar_data.copy()
from sklearn.model_selection import train_test_split
feat = new_data.drop("target",axis=1)
targ = new_data["target"]

feat   # Explanatory variables

targ   # response variable

"""## Dealing with imbalacing"""

targ.value_counts()

# Initially
plt.pie(targ.value_counts(), labels = ["not pulsar", 'pulsar'], colors = ['blue', 'red'], autopct = '%1.1f%%', shadow = True, explode = [0.2, 0.1])
plt.show()

df = .sample(frac=0.50)

# Manage the imbalance

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy="minority")

features,target= smote.fit_resample(feat, targ)
target.value_counts()

df = df.sample(frac=0.50)

target.shape

features.shape

# After dealing with imbalacing
plt.pie(target.value_counts(), labels = ["not pulsar", 'pulsar'], colors = ['blue', 'red'], autopct = '%1.1f%%', shadow = True, explode = [0.2, 0.1])
plt.show()

"""# ANN

## Splitting dataset into training and testing dataset

In this step, we are going to split our dataset into training and testing datasets. This is one of the bedrocks of the entire machine learning process. The training dataset is the one on which our model is going to train while the testing dataset is the one on which we are going to test the performance of our model.

Here we have used the **train_test_split** function from the **sklearn** library. We have split our dataset in a configuration such that **70** percent of data will be there in the training phase and 30 percent of data will be in the testing phase.
"""

# Splitting dataset into training and testing dataset
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42)

x_train

x_test.shape

"""## Performing Feature Scaling

The very last step in our feature engineering phase is feature scaling. It is a procedure where all the variables are converted into the same scale. Sometimes in our dataset, certain variables have very high values while certain variables have very low values.

We perform feature scaling after spliting the dataset into training and testing datasets. The reason being, the training dataset is something on which our model is going to train or learned itself. While the testing dataset is something on which our model is going to be evaluated. If we perform feature scaling before the train-test split then it will cause information leakage on testing datasets which neglects the purpose of having a testing dataset and hence we should always perform feature scaling after the train-test split.
"""

#Performing Feature Scaling

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

mean = x_train.mean(axis=0)
# Note that "train_data -= mean" is the same as "train_data = train_data - mean"
# The "/=" operation is the same but with division.
x_train -= mean
std = x_train.std(axis=0)
x_train /= std

x_test -= mean
x_test /= std

x_test

"""## Initializing Artificial Neural Network

Once we initialize our ann, we are now going to create layers for the same. Here we are going to create a network that will have 2 hidden layers, 1 input layer, and 1 output layer. So, let’s create our very first hidden layer.

For the second input, we are always going to use “relu”[rectified linear unit] as an activation function for hidden layers. Since we are going to create two hidden layers, this same step we are going to repeat for the creation of the second hidden layer as well.

Here we have created our first hidden layer by using the Dense class which is part of the layers module. This class accepts 2 inputs:-

1. units:- number of neurons that will be present in the respective layer

2. activation:- specify which activation function to be used

Here again, we are going to use the Dense class in order to create the output layer. For the binary classification Problems, the activation function that should always be used is sigmoid. For a multiclass classification problem, the activation function that should be used is softmax.
"""

from keras import models
from keras import layers

# Initialising ANN
ann = models.Sequential()
# Adding First Hidden Layer
ann.add(layers.Dense(units=16,activation="relu",input_shape=(x_train.shape[1],)))
# Adding Second Hidden Layer
ann.add(layers.Dense(units=16,activation="relu"))
# Adding Output Layer
ann.add(layers.Dense(units=1,activation="sigmoid"))
#Compiling ANN
ann.compile(optimizer="adam",loss="binary_crossentropy",metrics=['accuracy'])

ann_hist = ann.fit(x_train, y_train,
                                   epochs=20,
                                   batch_size=512,
                                   validation_data=(x_test, y_test))

epochs = range(1, 21)
original_val_loss = ann_hist.history['val_loss']

"""# L1 regularizer"""

from keras import models
from keras import layers
from keras import regularizers

l1_model = models.Sequential()
l1_model.add(layers.Dense(6, kernel_regularizer=regularizers.l1(0.001),
                          activation='relu', input_shape=(x_train.shape[1],)))
l1_model.add(layers.Dense(6, kernel_regularizer=regularizers.l1(0.001),
                          activation='relu'))
l1_model.add(layers.Dense(1, activation='sigmoid'))
l1_model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['acc'])

l1_model_hist = l1_model.fit(x_train, y_train,
                             epochs=20,
                             batch_size=512,
                             validation_data=(x_test, y_test))

l1_model_val_loss = l1_model_hist.history['val_loss']

def modelplot_L1():
  sns.scatterplot(x=epochs, y=original_val_loss, color="b", marker='x', label='Original model')
  sns.scatterplot(x=epochs, y=l1_model_val_loss, color="b", marker='o', label='L1-regularized model')
  plt.xlabel('Epochs')
  plt.ylabel('Validation loss')
  plt.legend()

modelplot_L1()

"""# L2 Regularizer"""

from keras import regularizers

l2_model = models.Sequential()
l2_model.add(layers.Dense(6, kernel_regularizer=regularizers.l2(0.001),
                          activation='relu', input_shape=(x_train.shape[1],)))
l2_model.add(layers.Dense(6, kernel_regularizer=regularizers.l2(0.001),
                          activation='relu'))
l2_model.add(layers.Dense(1, activation='sigmoid'))
l2_model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['acc'])

l2_model_hist = l2_model.fit(x_train, y_train,
                             epochs=20,
                             batch_size=512,
                             validation_data=(x_test, y_test))

l2_model_val_loss = l2_model_hist.history['val_loss']

def modelplot_L2():
  sns.scatterplot(x=epochs, y=original_val_loss, color="b", marker='x', label='Original model')
  sns.scatterplot(x=epochs, y=l2_model_val_loss, color="b", marker='o', label='L2-regularized model')
  plt.xlabel('Epochs')
  plt.ylabel('Validation loss')
  plt.legend()

modelplot_L2()

"""# Dropout"""

dpt_model = models.Sequential()
dpt_model.add(layers.Dense(6, activation='relu', input_shape=(x_train.shape[1],)))
dpt_model.add(layers.Dropout(0.5))
dpt_model.add(layers.Dense(6, activation='relu'))
dpt_model.add(layers.Dropout(0.5))
dpt_model.add(layers.Dense(1, activation='sigmoid'))

dpt_model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['acc'])

dpt_model_hist = dpt_model.fit(x_train, y_train,
                               epochs=20,
                               batch_size=512,
                               validation_data=(x_test, y_test))

dpt_model_val_loss = dpt_model_hist.history['val_loss']

def modelplot_Drop():
  sns.scatterplot(x=epochs, y=original_val_loss, color="b", marker='x', label='Original model')
  sns.scatterplot(x=epochs, y=dpt_model_val_loss, color="b", marker='o', label='Dropout-regularized model')
  plt.xlabel('Epochs')
  plt.ylabel('Validation loss')
  plt.legend()

modelplot_Drop()

